# 📓 Journal — September 20, 2025 — Week 2 Data Modeling & Testing

## 📋 Table of Contents
   1. [📘 What I Learned](#1--what-i-learned)
      - [🔧 Tools](#-tools)
      - [⚙️ Main Project Process](#️-main-project-process)
      - [📚 General Data Engineering Concepts](#-general-data-engineering-concepts)
   2. [📝 New Vocabulary](#2--new-vocabulary)
   3. [🧠 Data Engineering Mindset](#3--data-engineering-mindset)
   4. [🗂️ Decisions & Assumptions](#4--decisions--assumptions)
   5. [❓ Open Questions](#5--open-questions)
   6. [✅ Next Actions](#6--next-actions)
   7. [🔗 Artifacts & Links](#7--artifacts--links)
      - [📚 Data Engineering](#-data-engineering)
   8. [✨ Mini Reflection](#-mini-reflection)
   9. [🔥 BONUS: Meme of the Day](#-bonus-meme-of-the-day)


---

## 1) 📘 What I Learned


   ### ⚙️ Main Project Process

   **Documentation of last Saturday's project here for future reference:**
   * [September 20 Project Chinook Ingestion on Remote Server](  https://github.com/jynmargo/ftw-de-journal/blob/main/templates/personal/sept20-project-chinoook-ingestion.md)

   1. Data Pipeline Setup – Built DLT pipeline to extract Chinook data to ClickHouse
   2. Data Modeling – Changed naming, followed standars as instructed
   3. Created fact and dimension tables and connected to Metabase.


   ### 📚 General Data Engineering Concepts

   - Starting from scratch helps DEs to understand things better
      - Even if companies usually have DE / pipeline setups already
   - Important to confidently articulate the flow, how did you ingest, flow, aggregation
      - Talk in pipelines:
         Ingest to raw, clean, move into mart
      - ⚠️ In visualizations, check **Edge cases** (what looks sus? these are opportunities for trade offs)

   ### Sandbox
   - Ephemeral, meant to be exploratory
   - Table vs Views
   - Mechanical Data

   ### Group Assignment recap
   - When is it the right time to create views vs tables?
   #### DBeaver 
   - used to check the flow
   #### Metabase 
   - meant for end users only. Data engineers usually don’t use Metabase.
   - But if included in dbt script -> permanent (for prod setup)

   #### Misc Concepts:
   - Engine = Memory – shutdown memory -> data disappears
   - Saving data in sandbox ❌ not recommended
   - In DE, what's important is 'Did you do it? 🔍'
      - Better to include -> data mapping, flow, pipeline design on slide presentations (MOVEMENT OF DATA)
   - Documentation –> big part of data engineering (everything in docs)
   - What do you document?
   - Github is acquired by Microsoft
   - YAML
      - has been popular the Past 5 years
      - Data Modeling
         - How data is structured
         - OLTP vs OLAP
         - Data model in warehouse
      - Cubes -> pre-aggregation, predetermined SQL commands
      - RDBMS
         - (Mas pogi: ClickHouse)



   ### Business Perspective
   (Gives context on data)
	
   - When getting data, identify categories. Example: Female, Male, Prefer not to say -> What model are we trying to build?
   - If I were starting a business and selling something, what actions would I take to make it happen?
   - 'If there’s a customer who wants to buy. I need to make sure I can sell to them.'
   - 💰 Goal: Make the sale happen as fast as possible.

   ### Entity Relationship Model
   - Tables
   - Attributes
   - Relationships - SET OF BUSINESS RULES 
   - How SQL was born
      - with relational modeling ()
      - YAML - dictionary (rows and columns)
      - Graphs - represent data

   ### Normalization
   - Safety checks for dirty water 💧
   - 3RD NORMAL FORM
      - Non-key attributes should not depend on another non-key attributes
      - Gform -> try to model -> lalabas issues
   - 1NF 
      - atomic
   - 2NF
      - nonkey should depend on whole key

   ### Data Modeling layer
   - Conceptual -> Logical -> Physical
   - DE's work propagates -> if pangit model -> pangit upstream

   ### Modeling Approaches
   - Kimball / Dimensional - Fact + Dimensions for BI & Dashboards
   - Inmon / 3nf edw
   - Data Vault
   - Lakehouse / Medallion
   - Wide Table / OBT
   - Sir myk mentioned a 5-10k worth Dimensional modeling book
      (Sir Myk will recommend books soon, and I am curious to know more about them)





   
## 2) 📝 New Vocabulary

- Schema - modeling 
- Inmon - a top-down approach to data warehousing that creates a centralized, normalized Enterprise Data Warehouse (EDW)
- Kimball - a data warehousing approach that uses a dimensional model, typically a star schema, to provide business users with a fast, user-friendly, and understandable view of data for business intelligence and analytics

---

## 3) 🧠 Data Engineering Mindset

- Data Engineers
 - They should expect that things are meant to scale
 - They are meant to be used by a lot of people, which is why documentation is so important.
- ❗️Focus here: data mapping, flow, pipeline design. I keep forgetting this because having a ui/ux designing background makes me enjoy spending more time on designing and creating appealing visuals. That is not the job of a DE.
- DOCUMENT, DOCUMENT, DOCUMENT - I cannot stress this enough


---

## 4) 🗂️ Decisions & Assumptions

- Apart from the tools needed to be used, I need to study more on the business aspect to fully grasp how DEs take part in decision-making.

---

## 5) ❓ Open Questions

- What data are safe to ask AI or what data are safe to push on public github repositories?
(Safe to share: sample code, public datasets, open-source APIs, generic config templates.

Not safe to share:
   - Real usernames/passwords
   - API keys / tokens
   - Internal server IPs or connection strings (unless they’re just public demo servers like Chinook training DBs)
   - Proprietary business data

---

## 6) ✅ Next Actions

* [ ] Play around with sandbox schema 
* [ ] Create a documentation for the projects
* [ ] Spend some time on DSA in Python on Datacamp. Graduating in CS and forgetting it is a shame. I need to fix that before it haunts me.
* [ ] Know where DSA takes place in Data Engineering 

---

## 7) 🔗 Artifacts & Links

### 📚 Data Engineering

I have already listed these in my previous journal, but if you're here, I recommend checking them out too as these have helped me as well. :)
* [Chinook Database](https://github.com/lerocha/chinook-database)
* [Awesome Data Engineering resources](https://github.com/igorbarinov/awesome-data-engineering)
* [Dataset Repository](https://archive.ics.uci.edu/dataset/9/auto+mpg)

### ✍️ Markdown Tools

* [Markdown Preview](https://markdownlivepreview.com/)
* [Badge Generator](https://alexandresanlim.github.io/Badges4-README.md-Profile/#/?id=%e2%9a%a1-database-%f0%9f%94%9dmenu)




---

## ✨ Mini Reflection

As of now, I have experienced loading data in marts using dlt and dbt from raw csv files. 
I know that there are still a lot to learn on being a Data Engineer. I'm looking forward to the data testing, and trying out other ways to source data (such as using APIs), because it checks a life goal in my bucketlist, which is to work with APIs in data engineering (because it sounds complicated, and I find thrill in being able to challenge myself to do things I think that are hard.)

I really appreciate my groupmates in the last project we did, and I'm looking forward to doing more challenges with them (She-Ahn, ate Wish, ate CJ, & ate Rizza). They are very helpful, cooperative, and supportive, with each one having their own strengths upon us doing a workflow of the Auto_mpg dataset. I get to learn more stuff I don't know in data from the casual conversations we had and those are priceless.

I will start documenting the processes of projects starting now as I am inspired by Shi.ai and as Sir Myk keeps on reminding us.

Also as I have mentioned I still need to brush up my DSA understanding through this week's datacamp, as I might have forgotten a lot of it.

---

## 🔥 BONUS: Meme of the Day

Mikay thinks it's getting real now~ 

from her front-end/ui/ux/dashboarding comfort zone ✨🌈🦋🐤🐇  
to now exploring the deeper waters 👹 🐲🐙🦖

![hehehe](https://miro.medium.com/v2/1*cl5P-DYvLBW03XLRYiF6LQ.png)

Dashboards look simple on the surface.

But underneath, there’s a ton of invisible work (pipelines, cleaning, modeling) done by data engineers to make sure the dashboard shows something meaningful and trustworthy.

