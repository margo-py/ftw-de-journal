# 📓 Journal — September 27, 2025 — Week 3

## 📋 Table of Contents

1. [📘 What I Learned](#1--what-i-learned)

   * [⚙️ Main Project Process](#️-main-project-process)
   * [📚 General Data Engineering Concepts](#-general-data-engineering-concepts)
2. [📝 New Vocabulary](#2--new-vocabulary)
3. [🧠 Data Engineering Mindset](#3--data-engineering-mindset)
4. [🗂️ Decisions & Assumptions](#4--decisions--assumptions)
5. [❓ Open Questions](#5--open-questions)
6. [✅ Next Actions](#6--next-actions)
7. [🔗 Artifacts & Links](#7--artifacts--links)
8. [✨ Mini Reflection](#-mini-reflection)
9. [🔥 BONUS: Meme of the Day](#-bonus-meme-of-the-day)

---

## 1) 📘 What I Learned

* Learned to **ingest raw data with Python** and move it into structured models using DLT.
* Understood **why star schemas simplify reporting** and are better for queries, while **normalization protects integrity**.
* Discovered that **data quality checks must be embedded early** (aggregations, comparisons, null checks).
* Saw how small design choices (naming, table structure) ripple downstream and create big issues.

### ⚙️ Main Project Process

* Built end-to-end ingestion -> cleaning -> star schema.
* Iterated on schema design after mistakes (dropped/recreated dimension tables).
* On the afternoonm, we practiced the **Open University** dataset to apply both normalization and dimensional modeling.

### 📚 General Data Engineering Concepts

* **Raw != Ready**: dumping ground first, structured later.
* **Star schema is for clarity**: empowers non-engineers to query easily.
* **Snowflake schema is risky**: more joins = more complexity.
* **Multi-stage pipelines**: like guardrails—catch issues before they hit reports.

---

## 2) 📝 New Vocabulary

* **Star Schema** – simplified reporting model.
* **Snowflake Schema** – normalized but complex joins.
* **Normalization vs Standardization** – integrity vs consistency.

---

## 3) 🧠 Data Engineering Mindset

* Don’t aim for “perfect schema”—aim for **fit-for-purpose**.
* **Data integrity + usability** are both non-negotiable.

---

## 4) 🗂️ Decisions & Assumptions

* For **report-heavy cases -> star schema** is default.
* Assume dataset quality is uneven -> always build checks.
* Iteration is expected: schema will evolve with new insights.

---

## 5) ❓ Open Questions

* When is snowflake justified vs overengineering?
* What’s the “right level” of normalization before it slows performance?

---

## 6) ✅ Next Actions

* Apply pipeline thinking to Open University dataset.
* Try testing/validation scripts to catch missing values early.

---

## 7) 🔗 Artifacts & Links

* [OULAD DATASET](https://archive.ics.uci.edu/dataset/349/open+university+learning+analytics+dataset)

---

## ✨ Mini Reflection

Today's session helped me a lot understand what is it like to collaborate with teams. What are the factors needed to consider in terms of data normalization.
I also learned the value of high level communication to translate technical terms so that everyone or stakeholders can understand the topic being explained. (For example, we had to explain to the class with Ms Kara, wherein it was a good training ground to practincg presenting to panelists.)

---

## 🔥 BONUS: Meme of the Day

![Note](https://pbs.twimg.com/media/G0tHH4pbMAAHx9m.jpg)
*Me trying to identify the date format for the OULAP dataset on the Student Assessment table*

