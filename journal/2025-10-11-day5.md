# 📓 Journal - October 11, 2025 - Week 5 Web Scraping, Data Ethics, and Real-World Data Engineering Tips

## 📋 Table of Contents

1. [📘 What I Learned](#1--what-i-learned)

   * [🕸️ Data Collection & Web Scraping](#️-data-collection--web-scraping)
   * [📊 Data Shapes, Types & Formats](#-data-shapes-types--formats)
   * [💻 Web API Ingestion & Pandas](#-web-api-ingestion--pandas)
   * [⚖️ Data Ethics & Real-World Tips](#️-data-ethics--real-world-tips)
2. [📝 New Vocabulary](#2--new-vocabulary)
3. [🧠 Data Engineering Mindset](#3--data-engineering-mindset)
4. [🗂️ Decisions & Assumptions](#4--decisions--assumptions)
5. [❓ Open Questions](#5--open-questions)
6. [✅ Next Actions](#6--next-actions)
7. [🔗 Artifacts & Links](#7--artifacts--links)
8. [✨ Mini Reflection](#-mini-reflection)

---

## 1) 📘 What I Learned

### 🕸️ Data Collection & Web Scraping

🕘 *9:14 AM*

* The hardest topic so far.
* Sir Myk suggested not to solely rely on **web scraping** for capstone projects.
* Day 5 = Skills for Capstone + Web Scraping 101
* FTW Bootcamp is **not spoonfeeding** - the reality is job requirements vary, we’re pinpointing common skills.

**Lesson Proper:**

* **Agenda for Today:**

  * “Data in the Wild” -> collecting & handling data responsibly
  * Data works in a lifecycle:

    1. Something happens (Event)
    2. It’s represented as data (like temperature)
    3. Sensors measure it -> numbers
  * Example: Earthquake detection in Davao - sensors capture readings


**How websites work:**

* Browser (client) requests from server -> gets HTML back.
* Browser renders HTML visually; scraping = reading data, not viewing.
* Every website structure is different -> parsing needed.
* HTML = DOM tree 🌳 - BeautifulSoup parses that tree.
* JavaScript makes it dynamic, that’s where hidden APIs live (like Lazada).

**Web Scraping Levels:**

1. Static (HTML) -> `requests + BeautifulSoup`
2. Dynamic (JS/API) -> inspect Network -> XHR -> JSON
3. No API -> use **Headless browser (Playwright)**

---

### 📊 Data Shapes, Types & Formats

**Data as Different Shapes:**

* Tabular -> CSV/SQL
* Hierarchical -> JSON/XML
* Graph -> relationships (Neo4j)
* Time Series -> sensor data (Parquet/Influx)
* Geospatial -> map coords (GeoJSON, Shapefile)

> “Focus on the **flow** of the data, not the tools.” - Sir Myk

**Data Types & Semantics:**

* Primitive, Temporal, Categorical
* Depends on database - ex: Tableau or PowerBI use “modeling” (location, date).
* Representation affects efficiency - string timestamps = heavy.
* CSV = readable but large; JSON = flexible; Parquet = analytics-ready.
* Analysts love CSVs, but engineers think in **pipelines** - automation, scalability.

---

### 💻 Web API Ingestion & Pandas

**Afternoon Session:**

* Virtual env (`venv`) keeps environment isolated.
* Sir Myk says **UV** is now the best package manager.
* Pandas = best friend for small data work.
* SQL = foundation of data engineering.
* Extract small slices (use LIMIT, filters).
* Validate row counts and schemas.
* Pandas loads everything into memory (inefficient for GB-scale).
* Use **Ibis** for larger data -> same syntax, runs efficiently.

**APIs:**

* REST API = machines talking to machines.
* Look for “endpoints” and pass credentials if required.
* Learn to read API docs and watch for pagination, rate limits, retries.
* Always log requests with timestamps and durations.
* Example API: [Open Meteo](https://open-meteo.com/) - fetch temperature in Manila 🌡️

**Key Insight:**

> “If you can open it in Chrome, you can automate it - but websites fight back.”
> Use APIs when possible. Web scraping is the last resort.

---

### ⚖️ Data Ethics & Real-World Tips

* Just because we *can* collect, doesn’t mean we *should*.
* Data ethics = unseen but crucial.
* Collect with **consent and awareness.**
* Balance business goals with privacy, safety, dignity.
* **Shared responsibility**: data collection, transformation, reporting.
* Avoid: lack of transparency, ignoring bias.
* Bias isn’t always bad - just know where it exists.

**Real World Tips:**

1. Continuous learning
2. Build a portfolio
3. Find your focus (DE/DA)
4. Practice explaining data clearly
5. Build a career - not just a project

---

## 2) 📝 New Vocabulary

* **Headless Browser:** browser with no GUI (Playwright)
* **API Endpoint:** data access URL
* **Pagination:** API’s next/offset system for pages
* **Data Minimization:** only collect what’s needed
* **Sensor Data:** raw values measured from real-world events

---

## 3) 🧠 Data Engineering Mindset

* Mistakes are allowed - they’re part of growth.
* Logic and creativity work together.
* I respect logic and math deeply, but creativity keeps me grounded.
* The goal is **trustworthy, ethical, and resilient** data pipelines.

---

## 4) 🗂️ Decisions & Assumptions

* Prefer APIs over scraping whenever possible.
* If scraping, use **Playwright headless mode** for practice only.
* Store data ethically and document sources.
* Use Pandas for small datasets, Ibis or SQL for scale.

---

## 5) ❓ Open Questions

* When is web scraping legally and ethically okay?
* How to manage bias transparently in datasets?
* When should I transition from Pandas to Ibis or SQL?

---

## 6) ✅ Next Actions

* [ ] Practice Playwright for dynamic scraping (Lazada example)
* [ ] Try connecting Open Meteo API -> save JSON -> CSV
* [ ] Add logging for API requests (timestamp + duration)
* [ ] Explore Ibis and DuckDB for scalable data analysis
* [ ] Start documenting capstone data flow early

---

## 7) 🔗 Artifacts & Links

* [BeautifulSoup GitHub](https://github.com/wention/BeautifulSoup4)
* [Playwright Docs](https://playwright.dev/python/)
* [Ibis Project](https://ibis-project.org/)
* [Pandas IO Docs](https://pandas.pydata.org/docs/user_guide/io.html)
* [Open Meteo API](https://open-meteo.com/)


---

## ✨ Mini Reflection

I realized I’m actually growing a lot. I am also slowly realizing where my skills could fit in the big picture of Data Engineering or anything related. I enjoy building stuff, but struggle with the nitty-gritty. I think I may be doing best in building and setting up the tools, but as for the data cleaning and spotting data quality issues, this is an area I need to work on more.

Also, I used to think my creativity didn’t belong in data work, but I was wrong.
Data Engineering *needs* creativity - from building pipelines to solving real-world problems.

It’s okay to make mistakes.
Every bug and failed scrape taught me something.

> “Focus on the flow - it’s easier, not the tools.”
> “Web scraping is the last resort - APIs are your friends.”

I’m proud that I stayed awake despite not having any sleep last night, and learned a lot this today.

![](https://preview.redd.it/how-to-benefit-from-lean-data-quality-v0-fhm25s6gtuzd1.jpeg?auto=webp&s=7a9f40339bdc2e33ec1c6312008b44c33a5d527f)

